{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "The API provides both a [Unity ml-agents](https://github.com/Unity-Technologies/ml-agents) and an [OpenAI Gym](https://github.com/openai/gym) interface. We include training examples for both in [the examples folder](./examples); the former uses ml-agents' own training library which is optimised for the environment, the latter uses [OpenAI baselines](https://github.com/openai/baselines).\n",
    "\n",
    "\n",
    "In this notebook we show you how to run the `animal-ai` trainers which are optimized for training on the AnimalAI environment. It's a powerful modular library you can tinker with in order to implement your own algorithms. We strongly recommend that you have a look at its various parts (described at the end of this tutorial) should you wish to make modifications.\n",
    "\n",
    "## Can your agent self control? - Part II\n",
    "\n",
    "If you haven't done so already, go through the environement tutorial where we decribe the problem of self-control in animals. We created a curriculum which includes increasingly difficult levels in which the agent must retrieve food, while being introduced to objects similar to those in the final experiment, without encountering the exact testing configuration(s).\n",
    "\n",
    "Having created a curriculum in the previous notebook, we now need to configure the training environment. The `animalai-train` library provides all the tools you'll need to train using PPO or SAC - we'll be using the former here.\n",
    "\n",
    "First, we need to set all the hyperparameters of our model, which is done by creating a yaml file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnimalAI:\n",
      "    trainer: ppo\n",
      "    epsilon: 0.2\n",
      "    lambd: 0.95\n",
      "    learning_rate: 3.0e-4\n",
      "    learning_rate_schedule: linear\n",
      "    memory_size: 128\n",
      "    normalize: false\n",
      "    sequence_length: 64\n",
      "    summary_freq: 10000\n",
      "    use_recurrent: false\n",
      "    vis_encode_type: simple\n",
      "    time_horizon: 128\n",
      "    batch_size: 64\n",
      "    buffer_size: 2024\n",
      "    hidden_units: 256\n",
      "    num_layers: 1\n",
      "    beta: 1.0e-2\n",
      "    max_steps: 0.5e7\n",
      "    num_epoch: 3\n",
      "    reward_signals:\n",
      "        extrinsic:\n",
      "            strength: 1.0\n",
      "            gamma: 0.99\n",
      "        curiosity:\n",
      "            strength: 0.01\n",
      "            gamma: 0.99\n",
      "            encoding_size: 256\n"
     ]
    }
   ],
   "source": [
    "with open(\"configurations/training_configurations/train_ml_agents_config_ppo.yaml\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're already familiar with RL algorithms in general, these parameters should be fairly self-explanatory. Nonetheless, you can have a look at [this page](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Configuration-File.md) for explanations of the parameters specified for both PPO and SAC.\n",
    "\n",
    "You then need to configure the trainer, which is just a named tuple defining parameters such as:\n",
    "- the paths to the environment and your configuration file (above) \n",
    "- how many environments to launch in parallel and with how many agent per environment\n",
    "- the path to your curriculum\n",
    "- and many more!\n",
    "\n",
    "This is all done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from mlagents.trainers.trainer_util import load_config;\n",
    "from animalai_train.run_options_aai import RunOptionsAAI;\n",
    "from animalai_train.run_training_aai import run_training_aai;\n",
    "\n",
    "\n",
    "trainer_config_path = (\n",
    "    \"configurations/training_configurations/train_ml_agents_config_ppo.yaml\"\n",
    ")\n",
    "environment_path = \"env/AnimalAI\"\n",
    "curriculum_path = \"configurations/curriculum\"\n",
    "run_id = \"Agent_training_2\"\n",
    "base_port = 5005\n",
    "number_of_environments = 6\n",
    "number_of_arenas_per_environment = 12\n",
    "\n",
    "args = RunOptionsAAI(\n",
    "    trainer_config=load_config(trainer_config_path),\n",
    "    env_path=environment_path,\n",
    "    run_id=run_id,\n",
    "    base_port=base_port,\n",
    "    num_envs=number_of_environments,\n",
    "    curriculum_config=curriculum_path,\n",
    "    n_arenas_per_env=number_of_arenas_per_environment,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done we're pretty much just left with a one liner! The training library isn't verbose, but you can monitor training via Tensorboard. The first few lines just load tensorboard, once it is launched and you can see the orange window below, just click on the refresh button in the top right of Tensorboard - graphs will appear after a few training steps.\n",
    "\n",
    "_Note_: in case you don't want to wait for the model to train, you can jump ahead to the next step as we provide a pre-trained model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 10148), started 2:14:16 ago. (Use '!kill 10148' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-485b8ca5b6afacb7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-485b8ca5b6afacb7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting ./models/Agent_training_2/AnimalAI/frozen_graph_def.pb to ./models/Agent_training_2/AnimalAI.nn\n",
      "GLOBALS: 'is_continuous_control', 'version_number', 'memory_size', 'action_output_shape'\n",
      "IN: 'visual_observation_0': [-1, 84, 84, 3] => 'policy/main_graph_0_encoder0/conv_1/BiasAdd'\n",
      "IN: 'vector_observation': [-1, 1, 1, 3] => 'policy/main_graph_0/hidden_0/BiasAdd'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice'\n",
      "IN: 'action_masks': [-1, 1, 1, 6] => 'policy_1/strided_slice_1'\n",
      "OUT: 'policy/concat/concat', 'action'\n",
      "DONE: wrote ./models/Agent_training_2/AnimalAI.nn file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "logs_dir = \"summaries/\"\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {logs_dir}\n",
    "\n",
    "run_training_aai(0, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba del Modelo\n",
    "\n",
    "La siguiente celda permite cargar el modelo entrenado y ponerlo a prueba.\n",
    "\n",
    "**Integrantes**\n",
    "\n",
    "Álvaro Jimenez\n",
    "\n",
    "Andrés Quintero\n",
    "\n",
    "Jheyson Villavisan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\tf_utils\\tf.py:33: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\model_serialization.py:61: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\model_serialization.py:106: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\optimizer\\tf_optimizer.py:140: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\trainer_controller.py:66: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\trainer_controller.py:196: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\policy\\nn_policy.py:78: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\policy\\nn_policy.py:78: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\models.py:51: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\models.py:54: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\policy\\nn_policy.py:161: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\models.py:305: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\models.py:316: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\models.py:274: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\models.py:503: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\models.py:503: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\policy\\tf_policy.py:108: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\policy\\tf_policy.py:109: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\components\\reward_signals\\curiosity\\model.py:166: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\components\\reward_signals\\curiosity\\model.py:179: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\mlagents\\trainers\\models.py:75: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ae-qv\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Restoring parameters from ./models/Agent_training_2/AnimalAI\\model-2000108.ckpt\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from animalai.envs.arena_config import ArenaConfig\n",
    "from animalai_train.run_options_aai import RunOptionsAAI;\n",
    "from animalai_train.run_training_aai import run_training_aai;\n",
    "from mlagents.trainers.trainer_util import load_config;\n",
    "\n",
    "trainer_config_path = (    \"configurations/training_configurations/train_ml_agents_config_ppo.yaml\")\n",
    "environment_path = \"env/AnimalAI\"\n",
    "curriculum_path = \"configurations/curriculum\"\n",
    "run_id = \"Agent_training_2\"\n",
    "base_port = 5005\n",
    "number_of_environments = 6\n",
    "number_of_arenas_per_environment = 12\n",
    "\n",
    "args = RunOptionsAAI(\n",
    "    trainer_config=load_config(trainer_config_path),\n",
    "    env_path=environment_path,\n",
    "    run_id=run_id,\n",
    "    base_port=base_port+3,\n",
    "    load_model=True,\n",
    "    train_model=False,\n",
    "    arena_config=ArenaConfig(\"configurations/TEST/1-22-3.yml\")\n",
    ")\n",
    "run_training_aai(0, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if environment:\n",
    "    environment.close() # takes a few seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the agent get the reward about 50% of the time. It's far from perfect, but it's a good start! Remember, this problem is meant to be hard! You can now have a go at making your own algorithm to train agents that can solve one or more tasks in the `competition_configurations` folder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ML-Agents and AnimalAI for your algorithms\n",
    "\n",
    "As mentioned earlier, AnimalAI is built on top of ML-Agents, and we strongly recommend that you have a look at the various bits and pieces which you can tinker with in order to implement your own agents. This part provides a brief overview of where you can find these parts at the heart of most RL algortihms. We'll start from high level controllers and work our way down to the basic bricks of RL algorithms. Should you wish to modify them, you'll need to clone the [ml-agents repository](https://github.com/Unity-Technologies/ml-agents).\n",
    "\n",
    "- `animalai_train.run_training`: contains the highest level of control for training an agent. You can find all the subroutines you need in order to do so. The most import ones are:\n",
    "    - `animalai_train.subprocess_env_manager_aai.SubprocessEnvManagerAAI`: an environment manager which derives from `mlagents.trainers.subprocess_env_manager.SubprocessEnvManager` and can run multiple environments in parallel. In prcatice you shouldn't need to change this part.\n",
    "    - `mlagents.trainers.trainer_util.TrainerFactory`: a factory method which is in charge of creating trainer methods to manage the agents in the environment. In practice we only have a single type of agent in all of the environments, therefore there will only be one trainer to manage all the agents. **You might need to change this code** if you add a new RL algorithm, as it was designed to handle PPO and SAC.\n",
    "    - `animalai_train.trainer_controller_aai.TrainerControllerAAI`: derives from `mlagents.trainers.trainer_controller.TrainerController` and is where the training loop is.\n",
    "\n",
    "The basic elements which are most likely to be of interest to you:\n",
    "\n",
    "- **Curriculum**: managed in `animalai_train.meta_curriculum_aai.MetaCurriculumAAI` and `animalai_train.meta_curriculum_aai.CurriculumAAI`.\n",
    "- **RL algo**: you can find the implementations for PPO and SAC in `mlagents.trainers.ppo.trainer` and `mlagents.trainers.sac.trainer` respectively. They both implment the base class `mlagents.trainers.trainer.trainer` which you can implement and plug directly into the overall training setup (managing all the necessary model parameters in the `TrainerFactory` mentioned above).\n",
    "- **Exploration**: there is a curiosity module already provided in `mlagents.trainers.components.reward_signals`.\n",
    "- **Buffer**: the agent's replay buffer is in `mlagents.trainers.buffer`.\n",
    "\n",
    "There are many more components which you can find; two which are not implemented for AnimalAI, but which are on our todo list, are imitation learning and the option to record player actions in the environmnent.\n",
    "\n",
    "That's pretty much all there is to know, we hope you enjoy the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
